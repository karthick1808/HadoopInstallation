val df= Seq( 
(7369,"SMITH","CLERK",800,20) ,
(7499,"ALLEN","SALESMAN",1600,30),
(7521,"WARD","SALESMAN",1250,30),
(7566,"JONES","MANAGER",2975,20), 
(7654,"MARTIN","SALESMAN",1400,30),
(7698,"BLAKE",MANAGER",2850,30)
).toDF("ID","NAME","DEPT","SAL","LEAVES")

def catalog = 
      s"""{ 
         |"table":{"namespace":"default", "name":"emp"}, 
         |"rowkey":"empId", 
         |"columns":{ 
         |"key":{"cf":"rowkey", "col":"empId", "type":"int"}, 
         |"eName":{"cf":"person", "col":"eName", "type":"string"}, 
         |"job":{"cf":"person", "col":"job", "type":"string"}, 
         |"sal":{"cf":"person", "col":"sal", "type":"string"}, 
         |"deptno":{"cf":"person", "col":"deptno", "type":"string"}, 
         |} 
         |}""".stripMargin

case class HBaseRecord(
   col0: Int,
   col1: String,
   col2: String,
   col3: Int,
   col4: Int)

object HBaseRecord
{                                                                                                             
   def apply(i: Int, t: String): HBaseRecord = {
      val s = s"""row${"%03d".format(i)}"""       
      HBaseRecord(i,
      s"String$i: $t",
      s"String$i: $t"  
      i,
      i)
  }
}

val data = (0 to 255).map { i =>  HBaseRecord(i, "df")}

sc.parallelize(data).toDF.write.options(
 Map(HBaseTableCatalog.tableCatalog -> catalog, HBaseTableCatalog.newTable -> "5"))
 .format("org.apache.hadoop.hbase.spark ")
 .save()

def withCatalog(cat: String): DataFrame = {
  sqlContext
  .read
  .options(Map(HBaseTableCatalog.tableCatalog->cat))
  .format("org.apache.hadoop.hbase.spark")
  .load()
}
val df = withCatalog(catalog)