                                                      HADOOP HANDSON

REQUIREMENT 1:

1. created a folder called “samples” in the home directory of local machine
2.created a two files called “emp” and “dept” with the required data.
3. created a directory called “data” in HDFS:
                        hdfs dfs -mkdir /data
4. copied the samples folder to the data directory:
                  
                        hdfs dfs -copyFromLocal  /home/ak/samples  /data

I ) TO SEE THE CONTENT OF THE SAMPLES DIRECTORY:
                      
                         hdfs dfs -ls /data/samples/

21/01/30 11:27:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

-rw-r--r--   1 ak supergroup         97 2021-01-30 11:22 /data/samples/dept
-rw-r--r--   1 ak supergroup        401 2021-01-30 11:22 /data/samples/emp

II)  to see the first 5 employees from the “emp” file located in HDFS:

              hdfs dfs -cat /data/samples/emp | head -n 6
21/01/30 11:55:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
EMPNO,ENAME,JOB,SAL,DEPTNO
7369,SMITH,CLERK,800,20
7499,ALLEN,SALESMAN,1600,30
7521,WARD,SALESMAN,1250,30
7566,JONES,MANAGER,2975,20
7654,MARTIN,SALESMAN,1400,30       

III)   to remove the “dept “file located in HDFS:
                      
                       hdfs dfs   -rm   /data/samples/dept      

Deleted  /data/samples/dept

IV)   to copy the “emp” file from HDFS to the home directory in Unix
 
                         hdfs dfs -get /data/samples/emp /home/ak/

21/01/30 12:04:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

















==========================================================================================================================================================

Mapreduce
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.util.*;

public class readEmpHive{
    public static void main ( String[] args ) throws FileNotFoundException
    {
        Scanner scanner = new Scanner(new FileReader("D:/neuron.txt"));
        Map<String, ArrayList> myMap = new HashMap<String, ArrayList>();
        String[] keys = new String[]{"Creators","Developers","Quick Learners","Warriors","Travellers","MrPunctual"};
        int index=0;
        String line;

        while (scanner.hasNextLine())
        {
            scanner.nextLine();
            line = scanner.nextLine();
            if (!line.isEmpty())
            {
                String[] values = line.substring(4).split(", ");
               // System.out.println("Key : " + keys[index]);
                ArrayList al = new ArrayList<String>();
                for ( int i=0; i<values.length; i++)
                {
                 //   System.out.println("Value : " + values[i]);
                    al.add(values[i]);
                }
                myMap.put(keys[index], al);
                index++;
            }
        }
        System.out.println ( myMap );
    }
}

Save the program in a file named readEmpHive.java. Use the following commands to compile and execute this program.

javac readEmpHive.java
java readEmpHive

import java.io.IOException;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;

public class wordcount {

   public static class Map extends MapReduceBase implements Mapper <LongWritable, Text, Text,
  IntWritable> {
     private final static IntWritable one = new IntWritable(1);
     private Text word = new Text();

     public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, 
    Reporter reporter) throws IOException {
       String line = value.toString();
       StringTokenizer tokenizer = new StringTokenizer(line);
       while (tokenizer.hasMoreTokens()) {
         word.set(tokenizer.nextToken());
         output.collect(word, one);
       }
     }
   }

   public static class Reduce extends MapReduceBase implements Reducer<Text,IntWritable,Text, 
  IntWritable> {
   public void reduce(Text key, Iterator <IntWritable> values, 
    OutputCollector <Text, IntWritable>
 output, Reporter reporter) throws IOException {
       int sum = 0;
       while (values.hasNext()) {
         sum += values.next().get();
       }
       output.collect(key, new IntWritable(sum));
     }
   }

   public static void main(String[] args) throws Exception {
     JobConf conf = new JobConf(wordcount.class);
     conf.setJobName("wordcount");

     conf.setOutputKeyClass(Text.class);
     conf.setOutputValueClass(IntWritable.class);

     conf.setMapperClass(Map.class);
     conf.setCombinerClass(Reduce.class);
     conf.setReducerClass(Reduce.class);

     conf.setInputFormat(TextInputFormat.class);
     conf.setOutputFormat(TextOutputFormat.class);

     FileInputFormat.setInputPaths(conf, new Path(args[0]));
     FileOutputFormat.setOutputPath(conf, new Path(args[1]));

     JobClient.runJob(conf);
   }
}
=======================================================================================================================

                                                                           HIVE

Requirement 3:
1) To created the database called mydb:
                     create database mydb;

2) To create a table:

                       create table students(regno int,sname string,mark1 int,mark2 int,mark3 int)row format delimited fields terminated by ',';

3) To load the data into the table students
            load data local inpath '/home/ak/training/student.txt' into table              students;

Loading data to table mydb.students
OK
Time taken: 5.185 seconds

4) To see the content of the table:
 
                        select * from students;

OK
1000	'RAJ'	56	76	91
1001	'AMIT'	96	86	91
1002	'DINESH'	82	81	84
1003	'MAHESH'	77	78	79
1004	'DAVID'	86	86	81
1005	'JONES'	67	76	71
1006	'ANKIT'	88	83	98
1007	'ANAND'	89	87	94
Time taken: 7.029 seconds, Fetched: 8 row(s)

5) To see the schema of the table:
                    
                      describe students;

OK
regno               	int                 	                    
sname               	string              	                    
mark1               	int                 	                    
mark2               	int                 	                    
mark3               	int                 	                    
Time taken: 3.813 seconds, Fetched: 5 row(s)

6) to see the size and other properties of the table:
                   
                         how tblproperties students;



OK
numFiles	1
numRows	0
rawDataSize	0
totalSize	175
transient_lastDdlTime	1612009647
Time taken: 0.379 seconds, Fetched: 5 row(s)
===================================================================================================================

                                                     REQUIREMENT 4:
1) Created a table called EMP:
                        create table EMP(EMPNO int,ENAME string,JOB string,SAL string,DEPTNO string) row format delimited fields terminated by ',';

OK
Time taken: 4.273 seconds

2)To load the data into the a table EMP:
                     
       load data local inpath '/home/ak/samples/emp' into table EMP;

Loading data to table mydb.emp
OK
Time taken: 4.83 seconds

                                                     REQUIREMENT 5:


import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class readEmpHive 
{	private static String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver";
      	public static void main(String[] args) throws SQLException 
	{
		// Register driver and create driver instance
      		Class.forName(driverName);
      		// get connection
      		Connection con = DriverManager.getConnection("jdbc:hive://localhost:10000/MyDB", "", "");
		// create statement
		Statement stmt = con.createStatement();
		// execute statement
		Resultset res = stmt.executeQuery("SELECT * FROM emp;");
      
		System.out.println(" Empno \t EName \t Job \t Salary \t Deptno ");
                while (res.next())
                  {
                   System.out.println(res.getInt(1) + " " + res.getString(2) + " " + res.getString(3) + " " + res.getInt(4) + " " + res.getInt(5));
                  }
		con.close();
   	}
}
=======================================================================================================================

                                                     REQUIREMENT 6:
1)  create a table called qbank:
           create table qbank(word string);

2)To load data into qbank:
          load data local inpath ‘home/ak/bigdata/qbank.txt’ into table qbank;
3) To count the word in the file:
                select word,count(*) from qbank LATERAL VIEW explode(split(text, ' '))lTable as word group by word;
=============================================================================================================================
                                                   Requirement 7:  
1)To create a external table:
              create external table product(id int,product string,count int,price int) row format delimited fields terminated by ',' location '/product';

OK
Time taken: 0.683 seconds

2)To load data into the external table:
           load data local inpath '/home/ak/bigdata/product.txt' into table product;

Loading data to table mydb.product
OK
Time taken: 2.326 seconds
===========================================================================================================


                                              Requirement 8:

>>>create external table wiki_data (projectname string,pagename string,pageview int,pagesize int) row format delimited fields terminated by ' ' lines terminated by '\n' location '/home/cloudera/myData'

>>>load data local inpath '/home/cloudera/myData/browse_history1' into table wiki_data;

>>>load data local inpath '/home/cloudera/myData/browse_history2' into table wiki_data;

>>>create table wiki_en_views as select pagename,sum(pageview) from wiki_data where projectname="en" group by pagename;

==============================================================================================
                                              Requirement 9:
1) To create a EMP_DATA table:
                    create table EMP_data(EMPNO int,ENAMe string,job String,sal int,DEPTNO int) row format delimited fields terminated by ',';


OK
Time taken: 0.479 seconds

2)To load data into EMP_DATA:
            load data local inpath '/home/ak/bigdata/employee.txt' into table EMP_data;
   
Create a header file:
           set hive.exec.dynamic.partition=true;
           set hive.exec.dynamic.partition.mode=nonstrict;

3)To create a partition table:
    1) create a dummy table:
                      create table EMP_DATA1(EMPNO int,ENAMe string,job String,sal int) partitioned by (DEPTNO int) row format delimited fields terminated by ',';

     2) insert data into the dummy table:
                     insert into EMP_DATA1 partition(DEPTNO)   select EMPNO,ENAMe,job,sal,DEPTNO from EMP_data;


                











4)to display the employees from the 2nd partition that contains the employees of deptno 20.:
                     
                     select * from EMP_DATA1 where DEPTNO='20';

============================================================================================
                                              Requirement 10:

create table EMP (EMPNO int,ENAME varchar(20),JOB varchar(20),SAL int ,DEPTNO int) row format delimited fields terminated by ',' lines terminated by '\n'; 
create table DEPT (DEPTNO int,DNAME varchar(20),LOC varchar(20) ) row format delimited fields terminated by ',' lines terminated by '\n'; 

1)create view view_one as select  e.EMPNO,e.ENAME,e.JOB,e.SAL,d.DEPTNO  from EMP e join DEPT d on e.DEPTNO=d.DEPTNO where e.JOB='MANAGERS';
2) create view  details as select e.EMPNO,concat(e.ENAME d.MGR_NAME) as ENAME ,e.JOB,e.DEPTNO from EMP e join Manager d on e.DEPTNO=d.DEPTNO;
================================================================================================







               
                                                               sqoop
Requriement 1:
1)To transfer data from mysql to hdfs:
        sqoop import --connect jdbc:mysql://localhost:3306/mydb --username hiveuser --password hivepassword --m 1 --table emp --target-dir /empp
----------------------------------------------------------------------------------------------------------------------------------------------------------------



Requirement 2:

1) to load only the data of employees of manager
           sqoop import --connect jdbc:mysql://localhost:3306/mydb --username hiveuser --password hivepassword --m 1 --table emp --target-dir /empp

2) to view the data:
    hdfs dfs -cat /empp/part-m-00001
============================================================================================
requirement 3:

  1)To add new data with the table already exists in hdfs by append command:

          sqoop import --connect jdbc:mysql://localhost:3306/mydb --username hiveuser --password hivepassword --m 1 --table emp --append -m1
============================================================================================
Requirement 4:
 1)To load all tables present in the databse to the hdfs:
             sqoop import-all-tables --connect jdbc:mysql://localhost:3306/mydb --username hiveuser --password hivepassword --warehouse-dir '/vhp' --m 1;
============================================================================================

Requirement 5:

1) To load data form hdfs to rdms into table called product:
        sqoop export --connect jdbc:mysql://localhost/mydb --table product --export-dir /product/product.txt --username hiveuser --password hivepassword -m1 –input-fields-terminat-by ‘,’;

================================================================================================

Requirement 6:

1)sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/mydb --table product --username root  --password  cloudera  --hive-import --hive-table mydb.product --fields-terminated-by ',' -m1 
2)create a table in hive ->
            >>> create  external table product(product_id int,product_name varchar(45),product_qty int,product_price int) row format delimited fields terminated by ',' lines terminated by '\n';
  Then import the data from sql-->
            >>> sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/mydb --table product --username root  --password  cloudera  --hive-  import  --hive-table mydb.product --fields-terminated-by ',' -m1 
===================================================================================================


                                                                  HBASE:
                                                                Requirement 7:
1)	To create a table called users:
                        Create ‘users’, {Name=>’NAME’},{NAME=>’IMAGE’}
2)	To insert data into the table users:
                         Put ‘users’ , ’user1’ , ‘NAME:Fname’, ‘Mahesh’
                         Put ‘users’ , ’user1’ , ‘NAME:Lname’, ‘Kumar’
                         Put ‘users’ , ’user1’ , ‘IMAGE’, ‘Mahesh.jpg’
         Put ‘users’ , ’user2’ , ‘NAME:Fname’, ‘David’
         Put ‘users’ , ’user2’ , ‘NAME:Lname’, ‘john’
         Put ‘users’ , ’user2’ , ‘IMAGES’, ‘david.jpg
Put ‘users’ , ’user3’ , ‘NAME:Fname’, ‘Anand’
Put ‘users’ , ’user3’ , ‘NAME:Lname’, ‘Balaji’
Put ‘users’ , ’user2’ , ‘IMAGES’, ‘anand.jpg’
3)	To see the table content:
                 Scan ‘users’
===================================================================================================                         
                                              Requirement 8:
      hbase(main):016:0> scan'users'

ROW                   COLUMN+CELL
 user1                column=Image:, timestamp=1612077946689, value=Mahesh.jpg
 user1                column=Name:Fname, timestamp=1612077805160, value=Mahesh
 user1                column=Name:Lname, timestamp=1612077884906, value=Kumar
 user2                column=Image:, timestamp=1612078108363, value=David.jpg
 user2                column=Name:Fname, timestamp=1612078043316, value=David
 user2                column=Name:Lname, timestamp=1612078265854, value=john
 user3                column=Image:, timestamp=1612078317676, value=Anand.jpg
 user3                column=Name:Fname, timestamp=1612078166760, value=Anand
 user3                column=Name:Lname, timestamp=1612078277994, value=Balaji
 user4                column=Image:, timestamp=1612079071328, value=dravid.jpg
 user4                column=Name:Fname, timestamp=1612079006922, value=Dravid
 user4                column=Name:Lname, timestamp=1612079035890, value=raj
4 row(s) in 0.0700 seconds

2) TO see the first four rows:

hbase(main):017:0> scan'users',{STOPROW=>'user4'}

ROW                   COLUMN+CELL
 user1                column=Image:, timestamp=1612077946689, value=Mahesh.jpg
 user1                column=Name:Fname, timestamp=1612077805160, value=Mahesh
 user1                column=Name:Lname, timestamp=1612077884906, value=Kumar
 user2                column=Image:, timestamp=1612078108363, value=David.jpg
 user2                column=Name:Fname, timestamp=1612078043316, value=David
 user2                column=Name:Lname, timestamp=1612078265854, value=john
 user3                column=Image:, timestamp=1612078317676, value=Anand.jpg
 user3                column=Name:Fname, timestamp=1612078166760, value=Anand
 user3                column=Name:Lname, timestamp=1612078277994, value=Balaji
3 row(s) in 0.0430 seconds

3) To see the name coloum:
 
hbase(main):018:0> scan'users',{COLUMNS=>'Name'}
ROW                   COLUMN+CELL
 user1                column=Name:Fname, timestamp=1612077805160, value=Mahesh
 user1                column=Name:Lname, timestamp=1612077884906, value=Kumar
 user2                column=Name:Fname, timestamp=1612078043316, value=David
 user2                column=Name:Lname, timestamp=1612078265854, value=john
 user3                column=Name:Fname, timestamp=1612078166760, value=Anand
 user3                column=Name:Lname, timestamp=1612078277994, value=Balaji
 user4                column=Name:Fname, timestamp=1612079006922, value=Dravid
 user4                column=Name:Lname, timestamp=1612079035890, value=raj
4 row(s) in 0.0350 seconds

4)to see the content from 4 th row to 10th row:

hbase(main):020:0> scan'users',{STARTROW=>'user4',STOPROW=>'user10'}

                 ===================================================================================================



                                                                Requirement :9
1)	To delete the Lname value for the 2nd row:
                     delete ‘users’ , ‘user2’ , ‘NAME:Lname’
2)	To delete 3rd row from users table:
                          Deleteall ‘users’ , ‘user3’

3)	To remove all the rows from the table:
           Disable ‘users’
           Drop ‘users’
4)to check weather the table exists or not:
          Exists ‘users’
     ===========================================================================================================
                     

 Requirement :10
1)	To add new column into existing table:
                Alter ‘users’ ,NAME=>’EMAIL’
2)	To remove the existing table:
               Disable ‘users’
        Alter ‘users’ ,{NAME=>’EMAIL’,METHOD=>’delete’}

====================================================================================================
Requirement :11

  hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns="HBASE_ROW_KEY,mn:name,mn:id,mn:duration" -Dimporttsv.separator="," movie /inputdir/data/movies.dat
==============================================================================================
Requirement :12

(CREATING TABLE)
--------------
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;

public class MyfirstHBaseTable
{
  public static void main(String[] args) throws IOException
  {
    HBaseConfiguration hconfig = new HBaseConfiguration(new Configuration());
    HTableDescriptor htable = new HTableDescriptor("User"); 
    htable.addFamily( new HColumnDescriptor("Name"));
    htable.addFamily( new HColumnDescriptor("Image"));
    HBaseAdmin hbase_admin = new HBaseAdmin( hconfig );
    hbase_admin.createTable( htable );
  }
}

(LOADING DATA)


import java.io.IOException;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.util.Bytes;

public class PopulateHBaseTable
{
  public static void main(String[] args) throws IOException
  {

  org.apache.hadoop.conf.Configuration config = HBaseConfiguration.create();
  HTable table = new HTable(config, "User");
  Put p = new Put(Bytes.toBytes("user1"));
  p.add(Bytes.toBytes("Name"), Bytes.toBytes("Fname"),Bytes.toBytes("Mahesh"));
  p.add(Bytes.toBytes("Name"),Bytes.toBytes("Lname"),Bytes.toBytes("Kumar"));
  p.add(Bytes.toBytes("Image"),Bytes.toBytes("<mahesh.jpg>"));

  hTable.put(p);

  Put p1 = new Put(Bytes.toBytes("user2"));
  p1.add(Bytes.toBytes("Name"), Bytes.toBytes("Fname"),Bytes.toBytes("David"));
  p1.add(Bytes.toBytes("Name"),Bytes.toBytes("Lname"),Bytes.toBytes("John"));
  p1.add(Bytes.toBytes("Image"),Bytes.toBytes("<david.jpg>"));
  
  hTable.put(p1);

  Put p2 = new Put(Bytes.toBytes("user3"));
  p2.add(Bytes.toBytes("Name"), Bytes.toBytes("Fname"),Bytes.toBytes("Anand"));
  p2.add(Bytes.toBytes("Name"),Bytes.toBytes("Lname"),Bytes.toBytes("Balaji"));
  p2.add(Bytes.toBytes("Image"),Bytes.toBytes("<anand.jpg>"));
  
 // Saving the put Instance to the HTable.
  hTable.put(p2);
 // closing HTable
  hTable.close();
 }
}

=======================================================================================================================

                                                                           CASSANDRA

 REQUIREMENT 13:
 
$CASSANDRA_HOME
$Cassandra
sbin/cqlsh
CREATE KEYSPACE DEMO with replication ={'class':'SimpleStrategy','replication_factor':3};
use DEMO;
CREATE TABLE student ( regno int,sname varchar,mark1 int ,mark2 int,mark3 int ); /*varchar == text */
insert INTO student (regno,sname,mark1,mark2,mark3) values (1000,'RAJ',56,76,91;
insert INTO student (regno,sname,mark1,mark2,mark3) values (1001,'AMIT',96,86,91);
insert INTO student (regno,sname,mark1,mark2,mark3) values (1002,'DINESH',82,81,84);

select *from student where sname='AMIT';
================================================================================================================



                          
                                                                            Spark

REQUIREMENT 1:
    
To load data into spark:

                        var text=sc.textFile("file:///home/ak/fruit")
text: org.apache.spark.rdd.RDD[String] = file:///home/ak/fruit MapPartitionsRDD[12] at textFile at <console>:24

To see the content:
       
              text.collect
             res6: Array[String] = Array(id,name, 1,Apple, 2,Banana, 3,Orange, 4,Grapes)


To get the header:
 
                          var header=text.first()
header: String = id,name

to filter the header and show the content:

           var fin=text.filter(row=>row!=header)
<console>:1: error: illegal start of simple pattern

fin.collect

==========================================================================================



Requirement 2:

val data = sc.textFile("/user/cloudera/spark/")
val header = data.first() #extract header
val result= data.filter(row => row != header)
result.collect.foreach(println) 

==========================================================================================

Requirement 3:

val data = sc.textFile("/user/cloudera/inputdir/dataset/fruits.txt")
val header = data.first()
val rows= data.filter(row => row != header)
val final=rows.filter(w=>w(0)=='A')
val mapping=final.map(m=>(m,1))
val result=mapping.reduceByKey(+)
result.collect()
===========================================================================================

Requirement 4:

val sql = new org.apache.spark.sql.SQLContext(sc)
val s = sql.read.format("csv").option("header", "true").load("/home/cloudera/dataset/salessqlContext_tran.csv")
s.groupBy("CustID").count().show()
======================================================================================================

	REQUIREMENT 5 
======================================================================================================================================================================
      

  val states = Map(("BR","BIHAR"),("MH","MAHARASTRA"),("TN","TAMIL NADU"),("AP","ANDHRA PRADESH"),("KL","KERELA"))

  val broadcastStates = spark.sparkContext.broadcast(states)
  
  val data = Seq(("Amit","Mishra","BR"),
                 ("Nitin","Kulkarni","MH"),
                 ("Ram","Kumar","TN"),
                 ("Kesav","Prasad","AP"),
                 ("Biju","Joseph","KL"),
                 ("Ravi","Teja","AP")
                )

  val rdd = spark.sparkContext.parallelize(data)

  val rdd2 = rdd.map(f=>{
    val state = f._3
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullState)
  })

  println(rdd2.collect().mkString("\n"))

}



   									REQUIREMENT 6      
======================================================================================================================================================================

val array=sc.parallelize(Array(10,20,40,50,30))
val accum=sc.longAccumulator("Accumulator1")
array.foreach(x => accum.add(x))
accum.value

   									REQUIREMENT 7   
======================================================================================================================================================================
  

val input=sc.parallelize (Seq(("Chennai", "20000000"), ("Mumbai", "50000000"), ("Delhi", "40000000"))).toDF("City","humancount")
input.show()

   									REQUIREMENT 8     
======================================================================================================================================================================

val arrayData=Seq(
    ("Amit,,Mishra",List("Java","Scala","C++"),"UP"),
    ("Prabhu,Ram,",List("Spark","Java","C++"),"TN"),
    ("Ramesh,Kumar",List("CSharp","VB"),"TN")
  )
val arraySchema = new StructType()
    .add("name",StringType)
    .add("languagesAtSchool", ArrayType(StringType))
    .add("currentState", StringType)

val df = spark.createDataFrame(
    
sc.parallelize((arrayData),arraySchema)
df.printSchema()
df.show()


   									REQUIREMENT 9      
======================================================================================================================================================================

1)val data = sc.parallelize(Seq((7369,"SMITH","CLERK",800,20),
                                (7499,"ALLEN","SALESMAN",1600,30),
                                (7521,"WARD","SALESMAN",1250,30),
                                (7566,"JONES","MANAGER",2975,20),
                                (7654,"MARTIN","SALESMAN",1400,30),
                                (7698,"BLAKE",MANAGER",2850,30)
                                )).toDF("EMPNO", "ENAME", "JOB", "SALARY","DEPTNO")

2)data.withColumn("SALARY",col("SALARY")+100).show

3)data.withColumn("Bonus",col("SALARY")*0.2).show


   									REQUIREMENT 10     
======================================================================================================================================================================

1)val data = sc.parallelize(Seq((7369,"SMITH","CLERK",800,"Operations"),
                                (7499,"ALLEN","SALESMAN",1600,"Marketing"),
                                (7521,"WARD","SALESMAN",1250, "Marketing"),
                                (7566,"JONES","MANAGER",2975, "Operations"),
                                (7654,"MARTIN","SALESMAN",1400, "Marketing"),
                                (7698,"BLAKE",MANAGER",2850, "Marketing")
                                )).toDF("EMPNO", "ENAME", "JOB", "SALARY","DEPARTMENT")

2)data.groupBy("DEPARTMENT").sum("SALARY").withColumnRenamed("sum(SALARY)", "Tot_Salary").show


3) data.groupBy("DEPARTMENT","JOB").sum("SALARY").withColumnRenamed("sum(SALARY)", "Tot_Salary").show



4)data.groupBy("DEPARTMENT").sum("SALARY").withColumnRenamed("sum(SALARY)", "Tot_Salary").filter("Tot_Salary>=5000").show


   									REQUIREMENT 11 
======================================================================================================================================================================


1)val data = sc.parallelize(Seq((7369,"SMITH","CLERK",800,20),
             (7499,"ALLEN","SALESMAN",1600,30),
             (7521,"WARD","SALESMAN",1250,30),
             (7566,"JONES","MANAGER",2975,20),
             (7654,"MARTIN","SALESMAN",1400,30),
             (7698,"BLAKE",MANAGER",2850,30)
             )).toDF("EMPNO", "ENAME", "JOB", "SALARY","DEPTNO")

   val data1 = sc.parallelize(Seq((10,"ACCOUNTING","NEWYORK"),
                              (20,"RESEARCH","DALLAS"),
                              (30,"SALES","CHICAGO"),
                              (40,"OPERATIONS","BOSTON")
                              )).toDF("DEPTNO","DEPARTMENT","LOCATION")
   data.join(data1, "DEPTNO").show



2)data.join(data1,data("DEPTNO") ===  data1("DEPTNO"),"left").show


3)data.join(data1,data("DEPTNO") ===  data1("DEPTNO"),"right").show



   									REQUIREMENT 12
======================================================================================================================================================================


1)val data = sc.parallelize(Seq(ROW(7369,"SMITH","CLERK",800,20),
             (7499,"ALLEN","SALESMAN",1600,30),
             (7521,"WARD","SALESMAN",1250,30),
             (7566,"JONES","MANAGER",2975,20),
             (7654,"MARTIN","SALESMAN",1400,30),
             (7698,"BLAKE",MANAGER",2850,30)
             )).toDF("EMPNO", "ENAME", "JOB", "SALARY","DEPTNO")

2)import org.apache.spark.sql.expressions.WindowSpec
  val x = org.apache.spark.sql.expressions.Window.orderBy("SALARY")  
  val y = data.withColumn("Rank", rank().over(x))
   									REQUIREMENT 13 
======================================================================================================================================================================

1)val df= Seq( (7369,"SMITH","CLERK",800,20) ,
               (7499,"ALLEN","SALESMAN",1600,30),
               (7521,"WARD","SALESMAN",1250,30),
               (7566,"JONES","MANAGER",2975,20), 
               (7654,"MARTIN","SALESMAN",1400,30),
               (7698,"BLAKE",MANAGER",2850,30)
               ).toDF("ID","NAME","DEPT","SAL","LEAVES")

  df.groupBy("DEPT","NAME").max("SAL").show()

  df.groupBy("DEPT","NAME").min("SAL").show()


   									REQUIREMENT 14  
======================================================================================================================================================================

val df= Seq( 
(7369,"SMITH","CLERK",800,20) ,
(7499,"ALLEN","SALESMAN",1600,30),
(7521,"WARD","SALESMAN",1250,30),
(7566,"JONES","MANAGER",2975,20), 
(7654,"MARTIN","SALESMAN",1400,30),
(7698,"BLAKE",MANAGER",2850,30)
).toDF("ID","NAME","DEPT","SAL","LEAVES")

def catalog = 
      s"""{ 
         |"table":{"namespace":"default", "name":"emp"}, 
         |"rowkey":"empId", 
         |"columns":{ 
         |"key":{"cf":"rowkey", "col":"empId", "type":"int"}, 
         |"eName":{"cf":"person", "col":"eName", "type":"string"}, 
         |"job":{"cf":"person", "col":"job", "type":"string"}, 
         |"sal":{"cf":"person", "col":"sal", "type":"string"}, 
         |"deptno":{"cf":"person", "col":"deptno", "type":"string"}, 
         |} 
         |}""".stripMargin

case class HBaseRecord(
   col0: Int,
   col1: String,
   col2: String,
   col3: Int,
   col4: Int)

object HBaseRecord
{                                                                                                             
   def apply(i: Int, t: String): HBaseRecord = {
      val s = s"""row${"%03d".format(i)}"""       
      HBaseRecord(i,
      s"String$i: $t",
      s"String$i: $t"  
      i,
      i)
  }
}

val data = (0 to 255).map { i =>  HBaseRecord(i, "df")}

sc.parallelize(data).toDF.write.options(
 Map(HBaseTableCatalog.tableCatalog -> catalog, HBaseTableCatalog.newTable -> "5"))
 .format("org.apache.hadoop.hbase.spark ")
 .save()

def withCatalog(cat: String): DataFrame = {
  sqlContext
  .read
  .options(Map(HBaseTableCatalog.tableCatalog->cat))
  .format("org.apache.hadoop.hbase.spark")
  .load()
}
val df = withCatalog(catalog)


                                                                           

                                                                              SCALA

                                                                          REQUIREMENT 15
======================================================================================================================================================================
class Rational(n: Int, d: Int) {
    require(d != 0)
    private val g = gcd(n.abs, d.abs)
    val numer = n / g
    val denom = d / g
    override def toString = "Rational = "+numer +"/"+ denom
    private def gcd(a: Int, b: Int): Int = 
      if (b == 0) a else gcd(b, a % b)
  }
object HelloWorld {
   def main(args: Array[String]) {
       val r = new Rational(12, 8)
      println(r)
   }
}

                                                                     REQUIREMENT 16
======================================================================================================================================================================
class Rational(n: Int, d: Int) {
    require(d != 0)
    private val g = gcd(n.abs, d.abs)
    val numer = n / g
    val denom = d / g
    def this(n: Int) = this(n, 1)
    def + (that: Rational): Rational =
      new Rational(
        numer * that.denom + that.numer * denom,
        denom * that.denom
      )
    def + (i: Int): Rational =
      new Rational(numer + i * denom, denom)
    def - (that: Rational): Rational =
      new Rational(
        numer * that.denom - that.numer * denom,
        denom * that.denom
      )
    def - (i: Int): Rational =
      new Rational(numer - i * denom, denom)
    def * (that: Rational): Rational =
      new Rational(numer * that.numer, denom * that.denom)
    def * (i: Int): Rational =
      new Rational(numer * i, denom)
    def / (that: Rational): Rational =
      new Rational(numer * that.denom, denom * that.numer)
    def / (i: Int): Rational =
      new Rational(numer, denom * i)
    override def toString = "Rational = "+numer +"/"+ denom
    private def gcd(a: Int, b: Int): Int = 
      if (b == 0) a else gcd(b, a % b)
  }
object HelloWorld {
   def main(args: Array[String]) {
       val r = new Rational(12, 8)
        println(r*r)
   }
}


                                                                     REQUIREMENT 17
======================================================================================================================================================================
object MyClass {
     def main(args: Array[String]) = {  
        var ans = bigger(500,20);
        println(ans);
    }  
    def bigger(a:Int, b:Int):Int = {  
          if(a>b){
              a
          }  
          else{
              b
          }
    }  
}
                                                                    REQUIREMENT 18
======================================================================================================================================================================
object MyClass {
    def main(args: Array[String]) {
        var sum = 0
        for(arg<-args){
            sum = sum +arg.toInt
        }
        println(sum)
    }
}


                                                                     REQUIREMENT 19
======================================================================================================================================================================
object Scala {
    def main(args: Array[String]) {
        val phones_pr : List[String] = List("Nokia","Lava","Apple")
        val phones_np : List[String] = List("OnePlus","Microsoft","Mottorola","MI")
        val phones = phones_pr.concat(phones_np)
        val employee = ("Jaggu",1400000)
        val emname = employee._1
        val salary = employee._2
        var s : Set[String] = Set("Apple","Orange")
        s += "Banana"
        println(phones)
        println(emname)
        println(salary)
        println(s)
    }
}







                                                                     REQUIREMENT 20
======================================================================================================================================================================
object MyClass {
    def main(args: Array[String]) {
        var a =14;
        var b =42;
        while(a!=b){
            if(a>b){
                a=a-b;
            }
            else{
                b = b-a;
            }
        }
        println("The gcd of the two numbers is "+a);
    }
}

                                                                     REQUIREMENT 21
======================================================================================================================================================================

object MyClass {
     def reverseString(newString: String): String = {
        var revString = ""
        val n = newString.length()
        for(i <- 0 to n-1){
            revString = revString.concat(newString.charAt(n-i-1).toString)
        }
     revString
    }
    def main(args: Array[String]) {
        var newString = "Welcome"
        println(reverseString(newString))
    }
}
                                                                     REQUIREMENT 22
======================================================================================================================================================================
def getListOfFiles(dir: String):List[File] = {
    val d = new File(dir)
    if (d.exists && d.isDirectory) {
        d.listFiles.filter(_.isFile).toList
    } else {
        List[File]()
    }
}  
                                                                    REQUIREMENT 23
======================================================================================================================================================================
object Demo {
  var z = Array(2000, 1000, 500, 200, 100, 50, 20, 10, 1)
  var count: Int = 0
  def denomination(amount: Int) {
    var temp: Int = amount
    var den: Int = 0
    for (i <- 0 to z.length - 1) {
      count = temp / z(i)
      if (count >= 1) {
        println(z(i) + " x " + count + " = " + z(i) * count);
        temp = temp - z(i) * count
        den = den + count
      }
    }
    print("The minimum number of denomination for "+amount+" ="+den);
  }

  def main(args: Array[String]) {
    denomination(8941)
  }
}
 
                                                                     REQUIREMENT 24
======================================================================================================================================================================

object PrintTriangle {
def main(args: Array[String]) {
for(i <- 1 to 5){
for(j <- 1 to 5-i){
print(" ")
}
for(j <- 1 to i){
print(j);
}
for(j <- i-1 to 1 by -1)
{
print(j);
}
printf("\n");
}
}
}
                                                                     REQUIREMENT 25
======================================================================================================================================================================
import scala.io.Source  
  
// Creating object 
object gfgScala 
{  
    // Main method 
    def main(args:Array[String]) 
    {  
        val fname = "/uploads/Scalapractice.txt"
        val fSource = Source.fromFile(fname)
        var max : Int = 0
        var line_count : Int = 0
        var temp_line : String = "" 
        for(line<-fSource.getLines) 
        {
             if(line.length > max){
                 max = line.length
                 line_count = line_count+1
                 temp_line = line
             }
        } 
        println(max)
        println(line_count+1)
        println(temp_line)
        fSource.close()  
    }  
}








                                                                 REQUIREMENT 26
======================================================================================================================================================================
object Test extends App {
  def bubblesort(source: List[Int]) : List[Int]  = {
    
    def sort(iteration: List[Int], source: List[Int] , result: List[Int]) : List[Int]= source match {
      case h1 :: h2 :: rest => if(h1 > h2) sort(iteration, h1 :: rest, result :+ h2) else sort(iteration, h2 :: rest, result :+ h1) 
      case l:: Nil => sort(iteration, Nil, result :+ l)
      case Nil => if(iteration.isEmpty) return result else sort(iteration.dropRight(1), result, Nil )
    }
    sort(source,source,Nil)
  }
  println(bubblesort(List(4,3,2,224,15,17,9,4,225,1,7)))
}


                                                                       REQUIREMENT 27
======================================================================================================================================================================
object MyClass{
def main(args:Array[String])
{
val x = (1 to 100).toList
println(x)
val even_nums = x.filter(_ % 2 ==0)
println(even_nums)
val even_nu=even_nums.filter(_ > 50 )
println(even_nu)
val even_nu1=even_nums.filter(_ < 20 )
println(even_nu1)
val c = even_nu1 :::even_nu
println(c)
val d=c.map (  _ * 2 )
println(d)
}
}

                                                                  REQUIREMENT 28
======================================================================================================================================================================

object MyClass {
  def main(args: Array[String]) { 
      var myset =Set(1,2,3)
      println(myset)
      myset ++= List(10,20,30)
      myset +=5
      myset +=6
      myset +=7
      println(myset)
      var res =myset.filter(_>2)
      println(res)
      res -=10
      println(res)
   }
}

